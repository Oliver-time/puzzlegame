# src/puzzlegame/algorithms/behavioral_cloning.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os

class FeatureExtractionNet(nn.Module):
    """ç‰¹å¾æå–ç½‘ç»œï¼šä»ç¯å¢ƒèƒŒæ™¯ä¸­æå–å…³é”®ç‰¹å¾"""
    def __init__(self, input_dim=20, feature_dim=8):
        super(FeatureExtractionNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, feature_dim)  # æç‚¼ä¸º8ä¸ªå…³é”®ç‰¹å¾
        )
    
    def forward(self, x):
        return self.net(x)

class PuzzleNetFeatureBased(nn.Module):
    """åŸºäºç‰¹å¾æç‚¼çš„æ‹¼å›¾ç½‘ç»œ"""
    def __init__(self, bg_dim=20, puzzle_dim=3, pos_dim=1, 
                 feature_dim=8, hidden_dim=128, output_dim=3):
        super(PuzzleNetFeatureBased, self).__init__()
        
        # 1. ç¯å¢ƒç‰¹å¾æå–å™¨
        self.bg_feature_extractor = FeatureExtractionNet(bg_dim, feature_dim)
        
        # 2. æ‹¼å›¾ç‰¹å¾æå–å™¨ï¼ˆå¯é€‰çš„ï¼Œå¯ä»¥ç›´æ¥ç”¨ï¼‰
        self.puzzle_net = nn.Sequential(
            nn.Linear(puzzle_dim, 8),
            nn.ReLU()
        )
        
        # 3. ä½ç½®ç¼–ç å™¨
        self.pos_net = nn.Sequential(
            nn.Linear(pos_dim, 4),
            nn.ReLU()
        )
        
        # 4. ç‰¹å¾èåˆå’Œæ¯”è¾ƒå±‚
        # æ€»è¾“å…¥ï¼šç¯å¢ƒç‰¹å¾(8) + æ‹¼å›¾ç‰¹å¾(8) + ä½ç½®ç‰¹å¾(4) = 20
        self.fusion = nn.Sequential(
            nn.Linear(feature_dim + 8 + 4, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            # ç‰¹åˆ«æ³¨æ„ï¼šè¿™é‡Œæ·»åŠ ä¸€ä¸ª"åŒ¹é…åº¦"è¾“å‡ºå±‚
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, output_dim)
        )
        
        # 5. æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯é€‰ï¼‰ï¼šè®©ç½‘ç»œå…³æ³¨ç¯å¢ƒä¸­çš„å…³é”®ä½ç½®
        self.attention = nn.Sequential(
            nn.Linear(bg_dim + puzzle_dim + pos_dim, 16),
            nn.ReLU(),
            nn.Linear(16, bg_dim),
            nn.Softmax(dim=1)
        )
    
    def forward(self, x):
        # åˆ†å‰²è¾“å…¥
        bg = x[:, :20]          # èƒŒæ™¯éƒ¨åˆ†
        puzzle = x[:, 20:23]    # æ‹¼å›¾éƒ¨åˆ†
        pos = x[:, 23:]         # ä½ç½®éƒ¨åˆ†
        
        # æ–¹æ³•Aï¼šç›´æ¥ç‰¹å¾æå–
        bg_features = self.bg_feature_extractor(bg)  # ç¯å¢ƒç‰¹å¾ (batch, 8)
        puzzle_features = self.puzzle_net(puzzle)    # æ‹¼å›¾ç‰¹å¾ (batch, 8)
        pos_features = self.pos_net(pos)             # ä½ç½®ç‰¹å¾ (batch, 4)
        
        # æ–¹æ³•Bï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
        combined_input = torch.cat([bg, puzzle, pos], dim=1)
        attention_weights = self.attention(combined_input)  # (batch, 20)
        
        # åº”ç”¨æ³¨æ„åŠ›åˆ°èƒŒæ™¯ç‰¹å¾
        attended_bg = bg * attention_weights
        bg_features_attended = self.bg_feature_extractor(attended_bg)
        
        # èåˆæ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat([
            bg_features_attended,  # ä½¿ç”¨æ³¨æ„åŠ›ç‰ˆæœ¬
            puzzle_features,
            pos_features
        ], dim=1)
        
        # æœ€ç»ˆå†³ç­–
        output = self.fusion(combined_features)
        
        return output, attention_weights  # è¿”å›æ³¨æ„åŠ›å’Œä¾¿äºåˆ†æ


class PuzzleNetSimple(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥ç½‘ç»œç”¨äºæ‹¼å›¾åŠ¨ä½œé¢„æµ‹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    def __init__(self, input_dim, hidden_dim=128, output_dim=3):
        super(PuzzleNetSimple, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, output_dim)
        )

    def forward(self, x):
        return self.network(x)
    
def train_bc_model(data_path, model_save_path, n_epochs=100, 
                   batch_size=32, lr=1e-3, use_feature_based=True):
    """è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ BC æ¨¡å‹...")
    print(f"ğŸ“ è¯»å–æ•°æ®: {os.path.abspath(data_path)}")
    
    # --- æ•°æ®åŠ è½½ ---
    data = np.load(data_path)
    states = data['states']
    actions = data['actions']
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(states)} æ¡æ•°æ®ã€‚")
    
    # è·å–ç‰¹å¾ç»´åº¦
    input_dim = int(states.shape[1])
    output_dim = 3
    
    print(f"ğŸ§  è¾“å…¥ç»´åº¦: {input_dim}")
    
    # --- æ¨¡å‹åˆå§‹åŒ– ---
    if use_feature_based and input_dim == 24:
        print("ğŸ”§ ä½¿ç”¨ç‰¹å¾æç‚¼ç½‘ç»œ")
        model = PuzzleNetFeatureBased(
            bg_dim=20, 
            puzzle_dim=3, 
            pos_dim=1,
            feature_dim=8,
            hidden_dim=128, 
            output_dim=output_dim
        )
    else:
        print("ğŸ”§ ä½¿ç”¨ç®€å•ç½‘ç»œ")
        model = PuzzleNetSimple(
            input_dim=input_dim, 
            hidden_dim=128, 
            output_dim=output_dim
        )
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # --- æ•°æ®é¢„å¤„ç† ---
    states_tensor = torch.FloatTensor(states)
    actions_tensor = torch.LongTensor(actions).squeeze()
    
    dataset = torch.utils.data.TensorDataset(states_tensor, actions_tensor)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # --- è®­ç»ƒå¾ªç¯ ---
    print("â³ æ­£åœ¨è®­ç»ƒ...")
    for epoch in range(n_epochs):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_states, batch_actions in dataloader:
            optimizer.zero_grad()
            
            if use_feature_based:
                outputs, attention = model(batch_states)
            else:
                outputs = model(batch_states)
            
            loss = criterion(outputs, batch_actions)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs, 1)
            total += batch_actions.size(0)
            correct += (predicted == batch_actions).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total
        
        if (epoch+1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.1f}%")
            
            # å¯è§†åŒ–æ³¨æ„åŠ›ï¼ˆæ¯éš”ä¸€æ®µæ—¶é—´ï¼‰
            if use_feature_based and (epoch+1) % 30 == 0:
                visualize_attention(model, batch_states[:3])
    
    # --- ä¿å­˜æ¨¡å‹ ---
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    torch.save(model.state_dict(), model_save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {os.path.abspath(model_save_path)}")
    
    return model

def visualize_attention(model, sample_batch):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    model.eval()
    with torch.no_grad():
        _, attention_weights = model(sample_batch)
    
    print("\nğŸ” æ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰:")
    for i in range(min(3, len(sample_batch))):
        attention = attention_weights[i].numpy()
        print(f"æ ·æœ¬{i}:")
        print(f"  èƒŒæ™¯å€¼: {sample_batch[i, :20].numpy().round(2)}")
        print(f"  æ³¨æ„åŠ›: {attention.round(3)}")
        print(f"  é‡ç‚¹å…³æ³¨ä½ç½®: {np.where(attention > 0.1)[0]}")

class SimpleNet(nn.Module):
    def __init__(self, input_dim=2, middle_dim=32, output_dim=3):
        super(SimpleNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, middle_dim),
            nn.ReLU(),
            nn.Linear(middle_dim, output_dim)
        )
    def forward(self, x):
        return self.network(x)

def train_simple_model(data_path, model_save_path, n_epochs=100, batch_size=32, lr=1e-3):
    """è®­ç»ƒç®€å•æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€å•æ¨¡å‹...")
    print(f"ğŸ“ è¯»å–æ•°æ®: {os.path.abspath(data_path)}")
    
    # --- æ•°æ®åŠ è½½ ---
    data = np.load(data_path)
    states = data['states']
    actions = data['actions']
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(states)} æ¡æ•°æ®ã€‚")
    
    input_dim = 2
    output_dim = 3
    
    print(f"ğŸ§  è¾“å…¥ç»´åº¦: {input_dim}")
    
    # --- æ¨¡å‹åˆå§‹åŒ– ---
    model = SimpleNet(input_dim=input_dim, middle_dim=32, output_dim=output_dim)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # --- æ•°æ®é¢„å¤„ç† ---
    states_tensor = torch.FloatTensor(states)
    actions_tensor = torch.LongTensor(actions).squeeze()
    
    dataset = torch.utils.data.TensorDataset(states_tensor, actions_tensor)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # --- è®­ç»ƒå¾ªç¯ ---
    print("â³ æ­£åœ¨è®­ç»ƒ...")
    for epoch in range(n_epochs):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_states, batch_actions in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_states)
            loss = criterion(outputs, batch_actions)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs, 1)
            total += batch_actions.size(0)
            correct += (predicted == batch_actions).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total
        
        if (epoch+1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}, Acc: {accuracy:.1f}%")
    
    # --- ä¿å­˜æ¨¡å‹ ---
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    torch.save(model.state_dict(), model_save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {os.path.abspath(model_save_path)}")
    
    return model