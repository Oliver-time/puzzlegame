# src/puzzlegame/algorithms/behavioral_cloning.py

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os

class PuzzleNet(nn.Module):
    """ç®€å•çš„å…¨è¿æ¥ç½‘ç»œç”¨äºæ‹¼å›¾åŠ¨ä½œé¢„æµ‹"""
    def __init__(self, input_dim, hidden_dim=128, output_dim=3):
        super(PuzzleNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, output_dim)
        )

    def forward(self, x):
        return self.network(x)

def train_bc_model(data_path, model_save_path, n_epochs=100, batch_size=32, lr=1e-3):
    """è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒ BC æ¨¡å‹...")
    print(f"ğŸ“ è¯»å–æ•°æ®: {os.path.abspath(data_path)}")
    
    # --- æ•°æ®åŠ è½½ ---
    data = np.load(data_path)
    states = data['states']  # shape: (N, 24)
    actions = data['actions']
    
    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(states)} æ¡æ•°æ®ã€‚")
    
    # --- âœ… æ ¸å¿ƒä¿®å¤ï¼šè·å–ç‰¹å¾ç»´åº¦ï¼ˆç¬¬1ç»´ï¼‰---
    # states.shape[0] = æ ·æœ¬æ•° (895)
    # states.shape[1] = ç‰¹å¾æ•° (24) - è¿™æ˜¯æˆ‘ä»¬éœ€è¦çš„è¾“å…¥ç»´åº¦
    input_dim = int(states.shape[1])  # ä¿®å¤ï¼šä½¿ç”¨ shape[1] è€Œä¸æ˜¯æ•´ä¸ª shape
    output_dim = 3  # åŠ¨ä½œç©ºé—´ç»´åº¦ (å·¦/ä¸Š/å³)
    
    print(f"ğŸ§  æ„å»ºæ¨¡å‹: è¾“å…¥ç»´åº¦ = {input_dim} (ç±»å‹: {type(input_dim)})")
    
    # --- æ¨¡å‹åˆå§‹åŒ– ---
    model = PuzzleNet(input_dim=input_dim, hidden_dim=128, output_dim=output_dim)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # --- æ•°æ®é¢„å¤„ç† ---
    states_tensor = torch.FloatTensor(states)
    actions_tensor = torch.LongTensor(actions).squeeze() # ç¡®ä¿æ ‡ç­¾å½¢çŠ¶æ­£ç¡®
    
    dataset = torch.utils.data.TensorDataset(states_tensor, actions_tensor)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # --- è®­ç»ƒå¾ªç¯ ---
    print("â³ æ­£åœ¨è®­ç»ƒ...")
    model.train()
    for epoch in range(n_epochs):
        total_loss = 0
        for batch_states, batch_actions in dataloader:
            optimizer.zero_grad()
            outputs = model(batch_states)
            loss = criterion(outputs, batch_actions)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if (epoch+1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss/len(dataloader):.4f}")
    
    # --- ä¿å­˜æ¨¡å‹ ---
    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    torch.save(model.state_dict(), model_save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {os.path.abspath(model_save_path)}")