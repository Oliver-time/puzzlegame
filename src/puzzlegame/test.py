# src/puzzlegame/test.py

import os
import torch
import numpy as np

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

from puzzlegame.core.environment import PuzzleGame
from puzzlegame.algorithms.behavioral_cloning import PuzzleNet

def process_state(raw_state):
    # ... (ä¿æŒä¹‹å‰çš„å¤„ç†é€»è¾‘ä¸å˜ï¼Œç¡®ä¿ç»´åº¦ä¸º24) ...
    processed = []
    if isinstance(raw_state, dict):
        for value in raw_state.values():
            if isinstance(value, (list, np.ndarray)):
                processed.extend(value)
            else:
                processed.append(value)
    elif isinstance(raw_state, (list, np.ndarray)):
        processed = list(raw_state)
    else:
        processed = [raw_state]

    processed = np.array(processed)
    flat_state = processed.ravel()
    
    # å‡è®¾è®­ç»ƒç»´åº¦æ˜¯24
    expected_dim = 24
    if flat_state.size > expected_dim:
        return flat_state[:expected_dim]
    elif flat_state.size < expected_dim:
        padded = np.zeros(expected_dim)
        padded[:flat_state.size] = flat_state
        return padded
    return flat_state

def main():
    # --- âœ… ä¿®æ”¹ï¼šæŒ‡å‘æ–°è®­ç»ƒçš„åŠ æƒæ¨¡å‹ ---
    model_path = os.path.join(CURRENT_DIR, "data", "models", "bc_model_weighted.pth")
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        print("è¯·å…ˆè¿è¡Œ train_bc.py ç”Ÿæˆæ¨¡å‹")
        return

    model = PuzzleNet(input_dim=24, hidden_dim=128, output_dim=3)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    print(f"âœ… åŠ è½½åŠ æƒæ¨¡å‹æˆåŠŸ: {model_path}")

    env = PuzzleGame(n=20, m=3)
    raw_state = env.reset() 
    print(f"ğŸ® å¼€å§‹æ¸¸æˆæµ‹è¯•... ç›®æ ‡: ç§»åŠ¨ {env.m} ä¸ªæ–¹å—åˆ°å³ä¾§")

    # è¿ç»­åŠ¨ä½œè®¡æ•°å™¨ (è¾…åŠ©ç­–ç•¥ï¼ŒåŒé‡ä¿é™©)
    consecutive_same_action = 0
    last_action = -1
    done = False
    step = 0
    
    while not done:
        step += 1
        processed_state = process_state(raw_state)
        state_tensor = torch.FloatTensor(processed_state).unsqueeze(0)
        
        with torch.no_grad():
            logits = model(state_tensor)
            action_idx = torch.argmax(logits, dim=1).item()
        
        # --- è¾…åŠ©é€»è¾‘ï¼šé˜²æ­¢ç‰©ç†æ­»å¾ªç¯ ---
        if action_idx == last_action:
            consecutive_same_action += 1
        else:
            consecutive_same_action = 0
            last_action = action_idx

        # å¦‚æœè¿ç»­æ¨èåŒä¸€åŠ¨ä½œè¶…è¿‡é˜ˆå€¼ï¼Œå¼ºåˆ¶åœæ­¢ (å‡è®¾0æ˜¯åœæ­¢æˆ–å·¦ç§»)
        if consecutive_same_action >= 5:
            print(f"ğŸ›‘ è§¦å‘ç‰©ç†åˆ¹è½¦ï¼")
            action_to_take = 0 # å‡è®¾0æ˜¯å®‰å…¨åŠ¨ä½œ
        else:
            action_to_take = action_idx

        # æ‰§è¡Œç¯å¢ƒæ­¥è¿›
        result = env.step(action_to_take)
        if len(result) == 4:
            raw_state, reward, done, info = result
        elif len(result) == 5:
            raw_state, reward, done, _, info = result

        print(f"Step {step}: åŠ¨ä½œ={action_to_take} (æ¨¡å‹: {action_idx}), å¥–åŠ±={reward}, å®Œæˆ={done}")
        
        if step > 100:
            print("âš ï¸  è¶…è¿‡æœ€å¤§æ­¥æ•°")
            break

    if done and reward > 0:
        print("ğŸ‰ æ¨¡å‹æˆåŠŸå®Œæˆä»»åŠ¡ï¼")
    else:
        print("âŒ æ¨¡å‹æœªèƒ½å®Œæˆä»»åŠ¡")

if __name__ == "__main__":
    main()